{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Linear Regression\n",
    "\n",
    "The first exercise is about linear models.\n",
    "The given data set contains prices and other attributes of approximately 54,000 diamonds. You should fit a linear model to predict the price of a diamond, given its attributes.\n",
    "\n",
    "This exercise is meant to get you started with the tool stack. Besides numpy and matplotlib we use the following python packages:\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/)\n",
    "- [sklearn](http://scikit-learn.org/)\n",
    "\n",
    "If you are unfamiliar with them, follow the documentation links. \n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructor under\n",
    "\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "- maurice.wenig@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "- Deadline of submission:\n",
    "16.04.2024 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=54249)\n",
    "\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We use the same notation as in the lecture.\n",
    "- $m$... Number of datapoints\n",
    "- $n$... Number of features\n",
    "\n",
    "### Dataset \n",
    "\n",
    "As a dataset, we use the [diamond dataset](https://www.kaggle.com/shivam2503/diamonds).\n",
    "\n",
    "Each element in this dataset represents a diamond and has the following features:\n",
    "\n",
    "- price: price in US dollars (326.0 - 18823.0)\n",
    "- carat: weight of the diamond (0.2 - 5.01)\n",
    "- cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "- color: diamond colour, from J (worst) to D (best)\n",
    "- clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "- x: length in mm (0-10.74)\n",
    "- y: width in mm (0-58.9)\n",
    "- z: depth in mm (0-31.8)\n",
    "- depth: total depth percentage = 2 * z / (x + y) (43-79)\n",
    "- table: width of top of diamond relative to widest point (43-95)\n",
    "\n",
    "The dataset is stored under `diamonds.csv`.\n",
    "\n",
    "### Task 1\n",
    "Import the data from the file using [pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) and examine it.\n",
    "\n",
    "Determine the following:\n",
    "\n",
    "* The number of data points\n",
    "* The column names\n",
    "* The data types for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y', 'z']\n",
      "['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# TODO: load data (df = dataframe)\n",
    "df = pd.read_csv('diamonds.csv')\n",
    "\n",
    "# TODO: determine number of datapoints\n",
    "m = len(df)\n",
    "\n",
    "# TODO: determine column names\n",
    "column_names = df.columns.tolist()\n",
    "print(column_names)\n",
    "\n",
    "# TODO: determine datatypes of columns\n",
    "data_types = df.dtypes\n",
    "\n",
    "# assertions\n",
    "assert m == 53940\n",
    "assert set(column_names) == {\"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\"}\n",
    "assert data_types[\"carat\"] == \"float64\"\n",
    "assert data_types[\"cut\"] == \"object\"\n",
    "assert data_types[\"price\"] == \"int64\"\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Since there are discrete variables and we do not yet know how to include them into our regression model, remove them. Additionally, verify that there are no missing values in our dataset.\n",
    "\n",
    "Hint: there are multiple ways to [check](https://towardsdatascience.com/how-to-check-for-missing-values-in-pandas-d2749e45a345) for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cut', 'color', 'clarity'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [79], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# TODO: remove discrete variables + check for missing values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misnull()\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcut\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclarity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# assertions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m7\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['cut', 'color', 'clarity'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: remove discrete variables + check for missing values\n",
    "\n",
    "missing_values = df.isnull()\n",
    "df = df.drop(['cut', 'color', 'clarity'], axis=1)\n",
    "# assertions\n",
    "assert len(df.columns.values) == 7\n",
    "assert missing_values == True\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, we should **standardize** the data, to make different scales comparable.\n",
    "\n",
    "Standardization is defined for each feature $x_i$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{x}_i = \\cfrac{x_i-\\mu_x}{\\sigma_x}\\,,\n",
    "\\end{align}\n",
    "where $\\mu_x$ and $\\sigma_x$ are the empirical [mean](https://en.wikipedia.org/wiki/Mean) and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) of the feature $x$.\n",
    "\n",
    "### Task 3\n",
    "\n",
    "Convert the pandas dataframe to a numpy array and calculate the standardized data matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate standardized data matrix X\n",
    "x = None\n",
    "\n",
    "# assertions\n",
    "assert  x.shape == (53940, 7)\n",
    "assert np.allclose(np.mean(x, axis=0), 0)\n",
    "assert np.allclose(np.std(x, axis=0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Scikit learn has an [implementation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) of this preprocessing.\n",
    "\n",
    "Use it to create a second standardized data matrix and compare this result with your result from Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare to sklearn result\n",
    "x_sklearn = None\n",
    "\n",
    "# assertions\n",
    "assert np.allclose(x, x_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Data\n",
    "\n",
    "Visualizing correlation in your data often helps to build intuition and get a feeling of the deeper mojo in the set.\n",
    "\n",
    "Here we want to use the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) as a measure for correlation between two variables.\n",
    "\n",
    "Let $x$ and $y$ be two variables of the unstandardized dataset (e.g. `carat` and `price`). The empirical Pearson correlation coefficient between $x$ and $y$ is defined as \n",
    "\n",
    "\\begin{align}\n",
    "r_{xy} = \\cfrac{\\sum_{i=1}^m (x_i-\\bar{x}) (y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^m(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^m(y_i-\\bar{y})^2}}\\,,\n",
    "\\end{align}\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the respective empirical means.\n",
    "\n",
    "### Task 5\n",
    "\n",
    "How does this definition translate to our standardized data matrix $X$?\n",
    "\n",
    "Calculate the pairwise correlation matrix for our dataset. \n",
    "\n",
    "Visualize this correlation matrix and label the rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: calculate correlation matrix\n",
    "correlations = None\n",
    "\n",
    "# assertions\n",
    "row_0 = np.array([1., 0.02822431, 0.18161755, 0.9215913, 0.97509423, 0.9517222, 0.95338738])\n",
    "assert np.allclose(correlations[0], row_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize correlation matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Our goal in this exercise will be to predict the `price` of a diamond based on some of its other features.\n",
    "\n",
    "We will use linear regression, that is we assume the `price` (=$y$) depends linearly on the other features (=$\\mathbf{x}$):\n",
    "\\begin{align}\n",
    "y = \\theta^T \\mathbf{x} + \\varepsilon\n",
    "\\end{align}\n",
    "where $\\varepsilon$ is standard normal distributed noise.\n",
    "\n",
    "In `Linear_Regression_Script.pdf` you find how the maximum likelihood estimate $\\hat\\theta$ is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Implement a `LinReg` class that uses maximum likelihood estimation. Add the possibility to use [Ridge Regression](https://en.wikipedia.org/wiki/Ridge_regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Self\n",
    "import numpy.typing as npt\n",
    "\n",
    "\n",
    "class LinearRegressor():\n",
    "    def __init__(self, regularization_constant: int = 0):\n",
    "        \"\"\"Class for linear regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        regularization_constant : int, optional\n",
    "            Regularization constant, by default 0.\n",
    "        \"\"\"\n",
    "\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.theta: npt.NDArray[np.float64] = None\n",
    "\n",
    "    def fit(self, x: npt.NDArray[np.float64], y: npt.NDArray[np.float64]) -> Self:\n",
    "        \"\"\"Learns the parameters for a linear regression task.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : npt.NDArray[np.float64]\n",
    "            n_dim x n_data design matrix.\n",
    "        y : npt.NDArray[np.float64]\n",
    "            vector of length n_data.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: estimate theta\n",
    "        self.theta = None\n",
    "        return self\n",
    "\n",
    "    def predict(self, x: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "        \"\"\"Using learned parameters, predicts output for given X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : npt.NDArray[np.float64]\n",
    "            n_dim x n_data matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        npt.NDArray[np.float64]\n",
    "            n_data vector of predicted values.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: predict labels\n",
    "        return None\n",
    "\n",
    "\n",
    "# assertions\n",
    "np.random.seed(0)\n",
    "design_matrix = np.random.rand(2, 100)\n",
    "y = np.random.rand(100)\n",
    "regressor = LinearRegressor(regularization_constant=0.1).fit(design_matrix, y)\n",
    "assert np.allclose(regressor.theta, np.array([0.43078951, 0.43061373]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "First we want to predict the `price` of a diamond solely from the variable `carat`. \n",
    "Make a scatter plot of `carat` vs `price` using matplotlib. Label the axes and give the plot a title.\n",
    "\n",
    "Use the standardized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: display data in scatter plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Set up the design matrix and use your class to estimate $\\theta$ on the dataset.\n",
    "Note, that the design matrix does **not** need the vector of ones, since we standardized the dataset.\n",
    "\n",
    "Plot the regression line defined by $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build design matrix, y\n",
    "design_matrix = None\n",
    "y = None\n",
    "\n",
    "# TODO: use Linear Regression\n",
    "regressor = None\n",
    "\n",
    "# TODO: plot data + regression line\n",
    "\n",
    "# assertions\n",
    "assert design_matrix.shape == (1, 53940)\n",
    "assert np.isclose(regressor.theta[0], 0.9215913)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "You can find an implementation of this method in the python module [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Use it and compare your result for the estimation of $\\theta$.\n",
    "\n",
    "**Important:** scikit learn needs the design matrix as a $m \\times n$ matrix (datapoints as rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use scikit learn to estimate theta\n",
    "\n",
    "# TODO: compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Now predict the `price` from the variables `carat`, `depth`, `table`, `x`, `y`, `z`.\n",
    "\n",
    "- Estimate $\\theta$ with your class\n",
    "- Estimate $\\theta$ with scikit learn\n",
    "- Compare both estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build X, Y\n",
    "\n",
    "# TODO: estimate theta\n",
    "my_theta = None\n",
    "\n",
    "# TODO: estimate theta using scikit-learn + compare\n",
    "sklearn_theta = None\n",
    "\n",
    "# assertions\n",
    "assert np.allclose(my_theta, np.array([1.26970978, -0.07295331, -0.05737995, -0.36994281, 0.01898718, 0.00736359]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
