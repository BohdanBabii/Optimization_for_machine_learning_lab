{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0451f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group: Bohdan Babii, Felix KreÃŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7622934",
   "metadata": {},
   "source": [
    "# Exercise 3 - Binary Classification with Logistic Regression\n",
    "\n",
    "This exercise is meant to familiarize you with the complete pipeline of solving a machine learning problem. You\n",
    "need to obtain and pre-process the data, develop, implement and train a machine learning model and evaluate it\n",
    "by splitting the data into a train and testset.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructor under\n",
    "\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "- maurice.wenig@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "- Deadline of submission:\n",
    "30.04.2024 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=54249)\n",
    "\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c781c05",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the model of **logistic regression**, we have $m$ samples $x_i\\in\\mathbb{R}^n$ with labels $y_i\\in\\{-1,1\\}$.\n",
    "\n",
    "In this exercise, we will use the equivalent formulation with $y_i\\in\\{0,1\\}$.\n",
    "We use the example dataset `data.npy`, where we have 2 dimensional features (first two columns) and a binary label (3rd column).\n",
    "\n",
    "### Task 1\n",
    "Load and split the dataset into samples and labels. Then plot the data with a scatterplot and use different colors for different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de11b9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# TODO: Load and split dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m dataset[:, [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m dataset[:, \u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Load and split dataset\n",
    "dataset = np.load(\"data.npy\")\n",
    "x = dataset[:, [0,1]]\n",
    "y = dataset[:, 2]\n",
    "assert x.shape == (500, 2)\n",
    "assert y.shape == (500,)\n",
    "# TODO: plot the data\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Using different colors for different labels\n",
    "plt.scatter(x[y == 0][:, 0], x[y == 0][:, 1], c='red', label='Label 0')\n",
    "plt.scatter(x[y == 1][:, 0], x[y == 1][:, 1], c='blue', label='Label 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Scatter Plot of the Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c299ff",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09782e",
   "metadata": {},
   "source": [
    "\n",
    "The function $\\sigma$ is called the logistic *sigmoid function*:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(a) = \\cfrac{1}{1+\\exp(-a)}\\ .\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "###  Task 2\n",
    "Implement a vectorized logistic sigmoid function, i.e. it takes a vector of x-coordinates X and returns a vector of their respective y values. Use it to plot the function between -10 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3662c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.typing as npt\n",
    "def sigmoid(x: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    \"\"\"Sigmoid function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : npt.NDArray[np.float64]\n",
    "        Array of any dimension with scalar entries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npt.NDArray[np.float64]\n",
    "        Array of the same shape as the input with the sigmoid function applied element-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# TODO: Plot function from -10 to 10\n",
    "# Generate a range of values from -10 to 10\n",
    "X = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Apply the sigmoid function to these values\n",
    "Y = sigmoid(X)\n",
    "\n",
    "# Plot the function\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, Y, label=\"Sigmoid Function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "plt.title(\"Plot of the Sigmoid Function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c38493",
   "metadata": {},
   "source": [
    "The goal in logistic regression is to find the parameter vector $\\theta\\in\\mathbb{R}^n$, so that \n",
    "\n",
    "\\begin{align}\n",
    "p(y_i=1|x_i,\\theta)&=\\sigma(x_i^T\\theta)\\\\\n",
    "p(y_i=0|x_i,\\theta)&=1-p(y_i=1|x_i,\\theta)\n",
    "\\end{align}\n",
    "\n",
    "fits our data and can be used to predict the label on unseen data (binary classification).\n",
    "\n",
    "\n",
    "With an estimated $\\theta$, a new feature $x\\in\\mathbb{R}^n$ is classified according to:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = \\begin{cases}\n",
    "1&\\text{, if }p(y=1|x,\\theta)\\geq 0.5\\\\\n",
    "0&\\text{, else}\n",
    "\\end{cases}.\n",
    "\\end{align}\n",
    "\n",
    "Since $\\sigma(0) =  1/(1+\\exp(0)) = 0.5$. This is equivalent to \n",
    "\\begin{align}\n",
    "\\hat{y} = \\begin{cases}\n",
    "1&\\text{, if } x_i^T\\theta \\geq 0\\\\\n",
    "0&\\text{, else}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    " as noted in the lecture.\n",
    "\n",
    "### Task 3\n",
    "Prepare `x` so that the classification function for an estimated $\\theta$ is [*affine*](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function). Add this affine component at the **first column**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare x\n",
    "x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "assert x.shape == (500, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8bf503",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Implement a `predict` function based on the above definition of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e75f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: npt.NDArray[np.float64], theta: npt.NDArray[np.float64]) -> npt.NDArray[np.bool_]:\n",
    "    \"\"\"Predicts Y given X and theta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : npt.NDArray[np.float64]\n",
    "        Matrix with datapoints as rows (m x n).\n",
    "    theta : npt.NDArray[np.float64]\n",
    "        Parameter vector (n).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npt.NDArray[np.float64]\n",
    "        Vector of predictions (m).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement\n",
    "    return x @ theta >= 0\n",
    "\n",
    "\n",
    "# assertions\n",
    "np.random.seed(0)\n",
    "theta_assertion = np.array([-2, 0.5, 1])\n",
    "x_assertion = np.array([\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 3],\n",
    "    [1, 2, 2],\n",
    "    [1, 5, 0],\n",
    "    [1, 0, 2]\n",
    "])\n",
    "y_assertion = predict(x_assertion, theta_assertion)\n",
    "assert np.all(y_assertion == [False, False, False, True, True, True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320e271",
   "metadata": {},
   "source": [
    "## Learning $\\theta$\n",
    "\n",
    "For a given $\\theta$, we can calculate $p(y|x,\\theta)$ and use this probability for classification.\n",
    "To evaluate how well a learned $\\theta$ can be used to classify our data, we define a *loss function*.\n",
    "Here we want to use [binary cross entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) given as:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) = -\\cfrac{1}{m}\\sum_{i=1}^m y_i\\log(p(y_i=1|x_i,\\theta))+(1-y_i)\\log(1-p(y_i=1|x_i,\\theta))\n",
    "\\end{align}\n",
    "\n",
    "Often it is convenient to have multiple metrics at hand. In classification problems, the *accuracy* of a\n",
    "prediction is defined as the percentage of correctly classified features. In the case of logistic regression, this corresponds to \n",
    "\n",
    "\\begin{align}\n",
    "Acc(\\theta) = \\cfrac{1}{m}\\sum_{i=1}^m y_i \\hat{y_i} + (1-y_i)(1-\\hat{y_i})\n",
    "\\end{align}\n",
    "where $\\hat{y_i}$ is the prediction for $x_i$.\n",
    "\n",
    "As our model becomes better, we expect the accuracy to increase and the loss to decrease.  \n",
    "\n",
    "### Task 5\n",
    "Implement the binary cross entropy and the accuracy for logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(x: npt.NDArray[np.float64], y: npt.NDArray[np.float64], theta: npt.NDArray[np.float64]) -> float:\n",
    "    \"\"\"Computes the binary cross entropy loss of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : npt.NDArray[np.float64]\n",
    "        Matrix with datapoints as rows (m x n).\n",
    "    y : npt.NDArray[np.float64]\n",
    "        Vector of true labels (m).\n",
    "    theta : npt.NDArray[np.float64]\n",
    "        Parameter vector (n).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Binary cross entropy (scalar).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement\n",
    "    m = len(y)  # number of samples\n",
    "    probabilities = predict_probabilities(x, theta)\n",
    "    loss = - 1/m * (y @ np.log(probabilities + 1e-9) + (1 - y) @ np.log(1 - probabilities + 1e-9))\n",
    "    return loss\n",
    "\n",
    "def predict_probabilities(x: npt.NDArray[np.float64], theta: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    \"\"\"Predicts probabilities given X and theta, using the logistic sigmoid function.\"\"\"\n",
    "    return sigmoid(x @ theta)\n",
    "\n",
    "def accuracy(x: npt.NDArray[np.float64], y: npt.NDArray[np.float64], theta: npt.NDArray[np.float64]) -> float:\n",
    "    \"\"\"Computes the accuracy of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : npt.NDArray[np.float64]\n",
    "        Matrix with datapoints as rows (m x n).\n",
    "    y : npt.NDArray[np.float64]\n",
    "        Vector of true labels (m).\n",
    "    theta : npt.NDArray[np.float64]\n",
    "        Parameter vector (n).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy (scalar).\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    probabilities = predict(x,theta)\n",
    "    return 1/m * ((y @ probabilities.T) + ((1-y) @ (1-probabilities.T)))\n",
    "\n",
    "\n",
    "assert np.isclose(cross_entropy_loss(x, y, theta_assertion), 0.13431393626196375, atol=1e-2)\n",
    "assert np.isclose(accuracy(x, y, theta_assertion), 0.964)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3099ff",
   "metadata": {},
   "source": [
    "Given the loss function $L(\\theta)$, we want to minimize this function with respect to the parameters $\\theta$, that is we are looking for\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{argmin}_\\theta L(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "However, since this is a highly nonlinear optimization problem, we use an iterative approach that starts with an initial estimate for $\\theta$ and approaches the solution at each iteration step. \n",
    "The most simple approach is to take the gradient\n",
    "$\\nabla L(\\theta)$ of $L(\\theta)$ with respect to $\\theta$ and walk into direction of the negative gradient. \n",
    "This method is called gradient-descent.\n",
    "\n",
    "### Task 6\n",
    "\n",
    "Find out $\\nabla L(\\theta) = \\cfrac{d L}{d \\theta}$ and implement this function.\n",
    "The resulting function takes features $X$, labels $Y$ and $\\theta$ as input and outputs a gradient $\\nabla L(\\theta)\\in\\mathbb{R}^n$.\n",
    "\n",
    "Again, test your function with a randomly chosen $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(x: npt.NDArray[np.float64], y: npt.NDArray[np.float64], theta: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    \"\"\"Computes the gradient of the binary cross entropy loss with respect to theta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : npt.NDArray[np.float64]\n",
    "        Matrix with datapoints as rows (m x n).\n",
    "    y : npt.NDArray[np.float64]\n",
    "        Vector of true labels (m).\n",
    "    theta : npt.NDArray[np.float64]\n",
    "        Parameter vector (n).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npt.NDArray[np.float64]\n",
    "        Gradient at point theta (n).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement\n",
    "    m = x.shape[0] \n",
    "    probabilities = sigmoid(x @ theta)\n",
    "    gradient = -1 / m * x.T @ (y - probabilities)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# assertions\n",
    "assert np.allclose(cross_entropy_gradient(x, y, theta_assertion), [0.03666132, -0.05252688, -0.01899831], atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6b037",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "Implement the fit function for the following Logistic Regression class:\n",
    "\n",
    " 1. (randomly) choose initial $\\hat{\\theta}$\n",
    " 2. update $\\hat{\\theta} \\leftarrow \\hat{\\theta} -\\eta\\nabla L(\\hat{\\theta})$\n",
    " 3. repeat 2. until a maximum number of iterations $\\lambda$ (parameter `max_it`) is reached or the loss did not change more than $\\varepsilon$ (parameter `eps`).\n",
    " \n",
    "The hyperparameter $\\eta$ is also called *learning rate* (parameter `lr`).\n",
    "\n",
    "Additionaly track the losses and accuracies that occur during the iterations of gradient descend. \n",
    "\n",
    "Test your class (on the prepared data from above) and plot the accuracies and losses over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffefae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Self\n",
    "\n",
    "\n",
    "class LogisticRegressor():\n",
    "    def __init__(self, learn_rate: float = 1e-2, max_iterations: int = 1000, epsilon: float = 1e-5):\n",
    "        \"\"\"Regressor for binary classification using logistic regression. Fits using gradient descent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        learn_rate : float, optional\n",
    "            Learning rate, sets step size for descent, by default 1e-2.\n",
    "        max_iterations : int, optional\n",
    "            Maximum number of descent steps, by default 1000.\n",
    "        epsilon : float, optional\n",
    "            Descent stops early if the loss did not change more than this, by default 1e-5.\n",
    "        \"\"\"\n",
    "\n",
    "        self.learn_rate = learn_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "        self.theta = None\n",
    "        self.accuracies = []\n",
    "        self.losses = []\n",
    "\n",
    "    def predict(self, x: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "        \"\"\"Predicts Y given X and learned theta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : npt.NDArray[np.float64]\n",
    "            Matrix with datapoints as rows (m x n).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        npt.NDArray[np.float64]\n",
    "            Array of predictions (m).\n",
    "        \"\"\"\n",
    "\n",
    "        probabilities = predict_probabilities(x, self.theta)\n",
    "        return probabilities >= 0.5\n",
    "\n",
    "    def fit(self, x: npt.NDArray[np.float64], y: npt.NDArray[np.float64]) -> Self:\n",
    "        \"\"\"Gradient descent for binary crossentropy. Starts at a random parameter vector and tracks losses and accuracies along the iterations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : npt.NDArray[np.float64]\n",
    "            Matrix with datapoints as rows (m x n).\n",
    "        y : npt.NDArray[np.float64]\n",
    "            Vector of true labels (m).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement\n",
    "        self.theta = np.random.rand(x.shape[1])\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            current_loss = cross_entropy_loss(x, y, self.theta)\n",
    "            self.losses.append(current_loss)\n",
    "            gradients = cross_entropy_gradient(x, y, self.theta)\n",
    "            self.theta -= self.learn_rate * gradients\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = predict_probabilities(x, self.theta) >= 0.5\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            self.accuracies.append(accuracy)\n",
    "\n",
    "            # Check for convergence\n",
    "            if iteration > 0 and abs(self.losses[-2] - self.losses[-1]) < self.epsilon:\n",
    "                break\n",
    "                \n",
    "        return self\n",
    "\n",
    "\n",
    "# assertions\n",
    "np.random.seed(0)\n",
    "regressor = LogisticRegressor(max_iterations=5000, epsilon=0).fit(x, y)\n",
    "assert np.allclose(regressor.theta, [-2.77171607, 0.99485879, 1.07592613], atol=1e-2)\n",
    "\n",
    "# TODO: plot accuracy and loss\n",
    "np.random.seed(0)\n",
    "x_prepared = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "regressor = LogisticRegressor(max_iterations=5000, learn_rate=0.01, epsilon=1e-6).fit(x_prepared, y)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(regressor.losses, label='Loss')\n",
    "plt.title('Loss over iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(regressor.accuracies, label='Accuracy')\n",
    "plt.title('Accuracy over iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9fa24",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "So far, we used the whole dataset for fitting the `LogReg` class.\n",
    "\n",
    "- Use [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the dataset into train (75%) and testset (25%).\n",
    "- Fit the Logistic Regression model on the trainset and calculate the final accuracies on the train and testset. \n",
    "- Experiment with the hyperparameters for fit, to get a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e633e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Split data into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_prepared, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# TODO: apply logistic regression\n",
    "regressor = LogisticRegressor(max_iterations=5000, learn_rate=0.01, epsilon=1e-6)\n",
    "regressor.fit(x_train, y_train)\n",
    "\n",
    "# TODO: determine train and test accuracy\n",
    "train_accuracy = np.mean(regressor.predict(x_train) == y_train)\n",
    "test_accuracy = np.mean(regressor.predict(x_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d6109",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Next we want to visualize our classifier. To to this, we want to visualize the *decision boundary* defined by $\\hat{\\theta}$.\n",
    "\n",
    "The decision boundary is defined as \n",
    "$\n",
    "\\{x\\in\\mathbb{R}^n: p(y=1|x)=0.5\\}\n",
    "$\n",
    "or as in the lecture:\n",
    "$\\{x\\in \\{1\\} \\times \\mathbb{R}^n: x^T\\hat{\\theta}=0\\}$\n",
    "\n",
    "\n",
    "### Task 9\n",
    "\n",
    "Implement a function `plot_dec_boundary` that visualizes the data and the regression line for 2 dimensional samples $X$ and an estimated $\\hat{\\theta}$.\n",
    "\n",
    "Test this function with the $\\hat{\\theta}$ estimated in Task 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ebbcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_dec_boundary(x1: npt.NDArray, x2: npt.NDArray, y: npt.NDArray, theta: npt.NDArray):\n",
    "    \"\"\"Plots the decision boundary for 2D logistic regression task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : npt.NDArray\n",
    "        Matrix with datapoints as rows (m x 2).\n",
    "    y : npt.NDArray\n",
    "        Vector of true labels (m).\n",
    "    theta : npt.NDArray\n",
    "        Parameter vector (3).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: plot data and decision boundary\n",
    "    scatter = plt.scatter(x1, x2, c=y)\n",
    "    x1= np.linspace(np.min(x[:, 1]), np.max(x[:, 1]), 100)\n",
    "    x2= -(theta[0] + theta[1]*x1)/theta[2]\n",
    "    plt.plot(x1, x2, label= \"Decision Boundary\")\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend(*scatter.legend_elements())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# TODO: visualize the decision boundary you learned\n",
    "regressor.fit(x, y)\n",
    "print(regressor.theta)\n",
    "plot_dec_boundary(x[:, 1], x[:, 2], y, regressor.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ce71c",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Use the [implementation from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to train a logistic regressor.\n",
    "\n",
    "Visualize the regression line that you obtain with scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3547438",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: estimate theta with scikit-learn\n",
    "x = dataset[:, [0,1]]\n",
    "y = dataset[:, 2]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x, y)\n",
    "theta = np.hstack([model.intercept_, model.coef_.ravel()])\n",
    "\n",
    "\n",
    "# TODO: plot regression line with data\n",
    "\n",
    "plot_dec_boundary(x[:, [0]], x[:, [1]], y, theta)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b39a671858085d70125f4f28cb6256d6037eab582b2bde2ee45c6eac63f37da5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
