{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fcd20c",
   "metadata": {},
   "source": [
    "# Exercise 4 - Spam Filtering with Naive Bayes\n",
    "\n",
    "In this Exercise we will implement a spam detector that relies on the naive Bayes assumption. We will then compare its performance to logistic regression.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "- maurice.wenig@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "- Deadline of submission:\n",
    "14.05.2024 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=54249)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaee5db",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We will use the [SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download). You find this dataset as `spam_data.csv`. Each line consists of a message together with a label:\n",
    "- spam (message is a spam message)\n",
    "- ham (message is legitimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226b020",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Find a way to load the dataset and transform the features `X` (SMS) and the labels `Y` (spam/ham) into numerical representations.\n",
    "\n",
    "For transforming SMS into features, check out the bag of words representation from [scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# TODO: load data, transform into numerical features + labels\n",
    "df = pd.read_csv('spam_data.csv')\n",
    "vectorizer=CountVectorizer() \n",
    "x = vectorizer.fit_transform(df['messages']).toarray()\n",
    "y = np.where(df['labels'] == 'spam', 1, 0)\n",
    "# assertions\n",
    "assert x.shape == (5573, 8798)\n",
    "assert y.shape == (5573,)\n",
    "assert np.sum(x) == 80997"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6cd503",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "The naive Bayes filter is based on the Bayes formula with an additional simplifying (naive) assumption about the nature of the likelihood.\n",
    "\n",
    "Let $S = \\{\\text{spam}, \\text{ham}\\}$ be the source of a SMS and $W = [w_1,\\dots w_k]$ be the sequence of words contained in the SMS.\n",
    "Then filtering for spam and ham is done by evaluating the posterior distribution\n",
    "\\begin{align}\n",
    "p(S|W)&=\\cfrac{p(S)p(W|S)}{p(W)}\n",
    "\\end{align}\n",
    "\n",
    "Lets look at the single parts of the equations right hand side and how to implement them.\n",
    "\n",
    "As a running example we will use $W = [\\text{this}, \\text{is}, \\text{no}, \\text{spam}, \\text{message}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79137530",
   "metadata": {},
   "source": [
    "## Prior $p(S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805adc25",
   "metadata": {},
   "source": [
    "The prior distribution $p(S)$ is independent of the message $W$. We will use the maximum likelihood (ML) estimate for a categorical distribution, which is the relative frequency of the categories among the dataset.\n",
    "\\begin{align}\n",
    "p(S = \\text{spam})_{ML} &= \\cfrac{\\text{\\# SMS that are spam}}{\\text{\\# SMS}}\\\\\n",
    "p(S = \\text{ham})_{ML} &= 1 - p(S = \\text{spam})\n",
    "\\end{align}\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Estimate $p(S)$. Display the estimated distribution in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: estimate p(S), display in bar chart\n",
    "p_spam = np.sum(y)/ len(y)\n",
    "p_ham = 1-p_spam\n",
    "plt.bar(['spam', 'ham'],[p_spam, p_ham], color=['red', 'blue'])\n",
    "plt.xlabel('Klasse')\n",
    "plt.ylabel('Wahrscheinlichkeit')\n",
    "plt.show()\n",
    "# assertions\n",
    "assert np.isclose(p_spam, 0.13403911717207967)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01027f",
   "metadata": {},
   "source": [
    "## Likelihood p(W|S)\n",
    "\n",
    "The likelihood distribution models how likely a SMS is, given we know its either spam or ham.\n",
    "\n",
    "E.g. for $W = [\\text{this}, \\text{is}, \\text{no}, \\text{spam}, \\text{message}]$ we would expect something like \n",
    "\\begin{align}\n",
    "p(W|S=\\text{spam}) &= \\text{low}\\\\\n",
    "p(W|S=\\text{ham}) &= \\text{medium}\\\\\n",
    "\\end{align}\n",
    "\n",
    "However to estimate $p(W|S)$ we would need a dataset with the exact same $W$ appearing in both contexts: spam and ham. Since this is not the case for our dataset, this is the part where we make a naive assumption:\n",
    "\\begin{align}\n",
    "p(W|S) = \\prod_{w\\in W}p(w|S)\n",
    "\\end{align}\n",
    "That is, we consider each word in the SMS text independend of the others. This simplification enables us to estimate the likelihood, since single words to in fact appear in both contexts.\n",
    "\n",
    "For a single word $w$, we can again estimate the probability as relative frequency \n",
    "\\begin{align}\n",
    "p(w|S = \\text{spam})_{ML} &= \\cfrac{\\text{\\# word $w$ is labeled spam}}{\\text{\\# any word is labeled spam}}\\\\\n",
    "p(w|S = \\text{ham})_{ML} &= \\cfrac{\\text{\\# word $w$ is labeled ham}}{\\text{\\# any word is labeled ham}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Since we cannot expect every word to have appeared in a spam and ham message, we will smooth our dataset with a [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) of $\\varepsilon = 0.001$. \n",
    "\n",
    "This is done by adding $\\varepsilon$ to the count of every word in every SMS.\n",
    "\n",
    "As an example the count vector for a SMS over a vocabulary of 5 words is transformed from\n",
    "\\begin{align}\n",
    "[1, 2, 0, 0, 1]\n",
    "\\end{align}\n",
    "into\n",
    "\\begin{align}\n",
    "[1.001, 2.001, 0.001, 0.001, 1.001]\\,.\n",
    "\\end{align}\n",
    "\n",
    "This way we do not have zero probabilities in the product for calculating $p(W|S)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d60a6",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Apply Laplace smoothing ($\\varepsilon = 0.001$) to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply Laplace smoothing\n",
    "x_smooth = x+0.001\n",
    "# assertions\n",
    "assert np.isclose(np.sum(x_smooth), 80997 + 5573 * 8798 * 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e054c",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "For $W = [\\text{this}, \\text{is}, \\text{no}, \\text{spam}, \\text{message}]$, calculate $p(W|S)$.\n",
    "\n",
    "Display $p(w|S = \\text{spam}), p(w|S = \\text{ham})$ for every $w\\in W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a4d71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: calculate p(W|S) for W = [this is no spam message]\n",
    "test_message = [\"this\", \"is\", \"no\", \"spam\", \"message\"]\n",
    "spam_words= x_smooth[y==1].sum(axis=0)\n",
    "ham_words= x_smooth[y==0].sum(axis=0)\n",
    "p_test_word_given_spam= [spam_words[vectorizer.vocabulary_.get(w)]/np.sum(spam_words) for w in test_message]\n",
    "p_test_word_given_ham= [ham_words[vectorizer.vocabulary_.get(w)]/np.sum(ham_words) for w in test_message]\n",
    "p_test_message_given_spam = np.prod(p_test_word_given_spam)\n",
    "p_test_message_given_ham = np.prod(p_test_word_given_ham)\n",
    "# assertions\n",
    "assert np.isclose(p_test_message_given_spam, 7.31708303756972e-15)\n",
    "assert np.isclose(p_test_message_given_ham, 1.4235772856358894e-15)\n",
    "\n",
    "# TODO: plot p(w|S)\n",
    "fig, ax = plt.subplots(len(test_message), 1, sharey=False, figsize=(30, 30))\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_title('word: {}'.format(test_message[i]))\n",
    "    ax[i].bar(['p(w|S=spam)', 'p(w|S=ham)'],[p_test_word_given_spam[i], p_test_word_given_ham[i]], color=['red', 'blue'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f56062",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "From the dataset, list the top 5 words with the highest probabilities $p(w|S = \\text{spam})$ and $p(w|S = \\text{ham})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: list top 5 words accoding to p(w|S)\n",
    "words= vectorizer.get_feature_names_out()\n",
    "spam_df= pd.DataFrame({'word': words, 'count': spam_words})\n",
    "ham_df= pd.DataFrame({'word': words, 'count': ham_words})\n",
    "\n",
    "top_spam_words = spam_df.sort_values(by='count', ascending=False).head(5)['word']\n",
    "top_ham_words = ham_df.sort_values(by='count', ascending=False).head(5)['word']\n",
    "\n",
    "# assertions\n",
    "assert set(top_spam_words) == {\"to\", \"call\", \"you\", \"your\", \"free\"}\n",
    "assert set(top_ham_words) == {\"you\", \"to\", \"the\", \"and\", \"in\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023938d",
   "metadata": {},
   "source": [
    "## Evidence $p(W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0c342",
   "metadata": {},
   "source": [
    "The evidence tells us how likely the SMS was anyway. In many cases this is the most difficult part of the posterior to calculate. Here however we are lucky, since there are only two cases for $S$ and therefore\n",
    "\\begin{align}\n",
    "p(W) = p(W|S=\\text{spam}) + p(W|S=\\text{ham})\n",
    "\\end{align}\n",
    "\n",
    "That is $p(W)$ acts as a normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1948ddc",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "For $W = [\\text{this}, \\text{is}, \\text{no}, \\text{spam}, \\text{message}]$, calculate $p(W)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908824b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: calculate p(W) for W = [this is no spam message]\n",
    "p_test_message = p_test_message_given_spam + p_test_message_given_ham\n",
    "assert np.isclose(p_test_message, 8.740660323205609e-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a75eeb",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "With Prior, Likelihood and Evidence we can now assemble the Posterior\n",
    "\\begin{align}\n",
    "p(S|W)&=\\cfrac{p(S)p(W|S)}{p(W)}\\,.\n",
    "\\end{align}\n",
    "\n",
    "Remember that we want to use the Posterior to classify $W$:\n",
    "\\begin{align}\n",
    "\\kappa(W) = \\begin{cases}\n",
    "\\text{spam}&\\text{, if }p(S = \\text{spam}|W) \\geq p(S = \\text{ham}|W)\\\\\n",
    "\\text{ham}&\\text{, else}\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3f13e",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Implement the following `NaiveBayes` class. \n",
    "\n",
    "Use it to fit and predict on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2647be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.typing as npt\n",
    "from typing import Self\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, laplace_smoothing_constant: float = 0.0001):\n",
    "        \"\"\"Class for binary naive Bayes.\"\"\"\n",
    "\n",
    "        self.laplace_regularization_constant = laplace_smoothing_constant\n",
    "        # n_labels x n_words\n",
    "        self.log_p_word_given_label: npt.NDArray[np.float64] = None\n",
    "        # n_labels\n",
    "        self.log_p_label: npt.NDArray[np.float64] = None\n",
    "\n",
    "    def fit(self, x: npt.NDArray[np.float64], y: npt.NDArray[np.bool_]) -> Self:\n",
    "        \"\"\"Given a dataset of count vectors, calculates probabilities needed for prediction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : npt.NDArray[np.float64]\n",
    "            Word count matrix (n_sms x n_words).\n",
    "        y : npt.NDArray[np.bool_]\n",
    "            Label matrix (n_sms).\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: implement\n",
    "        self.log_p_label= [np.log(np.sum(y)/ len(y)), np.log(1-np.sum(y)/ len(y))]\n",
    "        x_smooth=x+self.laplace_regularization_constant\n",
    "        spam_words= x_smooth[y==1].sum(axis=0)\n",
    "        ham_words= x_smooth[y==0].sum(axis=0)\n",
    "        self.log_p_word_given_label= np.array([[np.log(spam_words[i]/np.sum(spam_words)) for i in range(x.shape[1])], \n",
    "                                      [np.log(ham_words[i]/np.sum(ham_words)) for i in range(x.shape[1])]])\n",
    "        return self\n",
    "\n",
    "    def predict(self, x: npt.NDArray[np.float64]) -> npt.NDArray[np.bool_]:\n",
    "        \"\"\"Given a dataset of count vectors, predicts labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : npt.NDArray[np.float64]\n",
    "            Word count matrix (n_sms x n_words).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        npt.NDArray[np.bool_]\n",
    "            Vector of predictions for labels (0 = ham, 1 = spam).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement\n",
    "        spam=[np.sum(self.log_p_word_given_label[0,x[i]!=0])+self.log_p_label[0] for i in range(x.shape[0])]\n",
    "        ham= [np.sum(self.log_p_word_given_label[1,x[i]!=0])+self.log_p_label[1] for i in range(x.shape[0])]\n",
    "        return [spam[i]>=ham[i] for i in range(len(spam))]\n",
    "        \n",
    "\n",
    "    def accuracy(self, x: npt.NDArray[np.float64], y: npt.NDArray[np.bool_]) -> float:\n",
    "        \"\"\"Calculates accuracy for given dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : npt.NDArray[np.float64]\n",
    "            Word count matrix (n_sms x n_words).\n",
    "        y : npt.NDArray[np.bool_]\n",
    "            Vector of true labels (0 = ham, 1 = spam).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Percentage of correctly classified x.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement\n",
    "        return np.sum(np.where(self.predict(x)==y, 1, 0))/len(y)\n",
    "\n",
    "\n",
    "# assertions\n",
    "classifier = NaiveBayesClassifier(laplace_smoothing_constant=0.0001).fit(x, y)\n",
    "assert classifier.accuracy(x, y) > 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c326d56",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Obviously we trained on the same dataset as we tested and therefore cannot quite judge the performance of the naive Bayes classifier. \n",
    "\n",
    "Split your data into 75% training- and 25% testdata. Use a seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# TODO: provide train + testsplit\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "assert np.isclose(len(x_train) / len(x_test), 3, atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23ca5e",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Now we systematically want to test our classifier. \n",
    "\n",
    "For different values of $\\varepsilon$ track the accuracy on train and testdata.\n",
    "Which value for $\\varepsilon$ would you recommend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74023c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: report train- and test accuracy for different epsilons\n",
    "epsilon= [0.000025*(i+1) for i in range(25)]\n",
    "for eps in epsilon:\n",
    "    classifier = NaiveBayesClassifier(laplace_smoothing_constant=eps).fit(x_train, y_train)\n",
    "    print(f'Train: {classifier.accuracy(x_train, y_train)}, Test: {classifier.accuracy(x_test, y_test)}, Epsilon: {eps}')\n",
    "\n",
    "# Das beste Ergebnis beim Training set bekommt man bei dem kleinsten epsilon und beim Test set bei eps = 0.00015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
